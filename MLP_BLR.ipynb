{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from functions import function\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dataset\n",
    "def plot_data(ax, X, Y):\n",
    "    plt.axis('off')\n",
    "    ax.scatter(X[:, 0], X[:, 1], s=1, c=Y, cmap='bone')\n",
    "    \n",
    "# plot the decision boundary of our classifier\n",
    "def plot_decision_boundary(ax, x_min, x_max, y_min, y_max, X, Y, classifier):\n",
    "    # forward pass on the grid, then convert to numpy for plotting\n",
    "    # Define the grid on which we will evaluate our classifier\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, .1),\n",
    "                        np.arange(y_min, y_max, .1))\n",
    "\n",
    "    to_forward = np.array(list(zip(xx.ravel(), yy.ravel())))\n",
    "    Z = []\n",
    "    for x in to_forward:\n",
    "        Z.append(classifier.forward(x, sample = True))\n",
    "    Z = np.array(Z)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # plot contour lines of the values of our classifier on the grid\n",
    "    ax.contourf(xx, yy, Z>0.5, cmap='Blues')\n",
    "    \n",
    "    # then plot the dataset\n",
    "    plot_data(ax, X,Y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_moons(n_samples=2000, noise=0.1)\n",
    "x_min, x_max = -1.5, 2.5\n",
    "y_min, y_max = -1, 1.5\n",
    "fig, ax = plt.subplots(1, 1, facecolor='#4B6EA9')\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "plot_data(ax, X, Y)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(object):\n",
    "    def forward(self, x, sample):\n",
    "        self.x = x\n",
    "        # print('ReLU shape',self.x.shape)\n",
    "        return np.maximum(0, x)\n",
    "        \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * (self.x > 0)\n",
    "    \n",
    "    def step(self, learning_rate):\n",
    "        pass\n",
    "\n",
    "class Sigmoid(object):\n",
    "    def forward(self, x, sample):\n",
    "        self.output = 1 / (1 + np.exp(-x))\n",
    "        # print('Sigmoid shape',self.output.shape)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * self.output * (1 - self.output)\n",
    "    \n",
    "    def step(self, learning_rate):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(object):\n",
    "    def __init__(self, n_input, n_output, sigma_w = None, sigma_b = None):\n",
    "        self.weights = np.random.randn(n_output, n_input)\n",
    "        self.bias = np.random.randn(n_output,1)\n",
    "        \n",
    "        if sigma_w is not None:\n",
    "            self.sigma_w = sigma_w\n",
    "        else:\n",
    "            self.sigma_w = np.eye(n_output*n_input)*1e-3\n",
    "        \n",
    "        if sigma_b is not None:\n",
    "            self.sigma_b = sigma_b\n",
    "        else:\n",
    "            self.sigma_b = np.eye(n_output)*1e-3\n",
    "\n",
    "    def forward(self, x, sample = True):\n",
    "        self.x = x.reshape(-1,1)\n",
    "        if sample:\n",
    "            self.weights =  np.random.multivariate_normal(mean = self.weights.flatten(), cov = self.sigma_w).reshape(self.weights.shape)\n",
    "            self.bias =  np.random.multivariate_normal(mean = self.bias.flatten(), cov = self.sigma_b).reshape(self.bias.shape)\n",
    "        return self.weights@self.x + self.bias\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        self.grad_weights = grad_output@self.x.T\n",
    "        self.grad_bias = np.sum(grad_output, axis = 1).reshape(self.bias.shape)\n",
    "        return self.weights.T@grad_output\n",
    "    \n",
    "    def step(self, learning_rate):\n",
    "        self.weights -= (np.linalg.pinv(self.sigma_w)@self.grad_weights.flatten()).reshape(self.weights.shape) * learning_rate\n",
    "        self.bias -= (np.linalg.pinv(self.sigma_b)@self.grad_bias.flatten()).reshape(self.bias.shape) * learning_rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(object):\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, x, sample):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x, sample)\n",
    "        return x\n",
    "    \n",
    "    def compute_loss(self, out, label):\n",
    "        BCE = -np.mean(label*np.log(out+1e-8)+(1-label)*np.log(1-out+1e-8))\n",
    "        self.grad_output = (-(label/(out+1e-10) - (1 - label) / (1 - out+1e-10))).reshape(-1,1)\n",
    "        return BCE\n",
    "\n",
    "    def backward(self):\n",
    "        grad_output = self.grad_output\n",
    "        # print('Begin backward ')\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_output = layer.backward(grad_output)\n",
    "\n",
    "\n",
    "    def step(self, learning_rate):\n",
    "        for layer in self.layers:\n",
    "            layer.step(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "batch_size = 100\n",
    "learning_rate = 1e-5\n",
    "\n",
    "\n",
    "h=50\n",
    "net = Sequential([Linear(2, h), ReLU(), Linear(h, 1), Sigmoid()])\n",
    "\n",
    "for it in trange(5000):\n",
    "    \n",
    "    # pick a random example id\n",
    "    batch_indices = np.random.choice(X.shape[0], batch_size, replace=False)\n",
    "\n",
    "    # select the corresponding example and label\n",
    "    loss = 0\n",
    "    for index in batch_indices:\n",
    "        example = X[index,:]\n",
    "        label = Y[index]\n",
    "        if it%11==0:\n",
    "            out = net.forward(example, sample = True)\n",
    "        else:\n",
    "            out = net.forward(example, sample = False)\n",
    "        loss += net.compute_loss(out, label)\n",
    "\n",
    "    # backward pass    \n",
    "    losses.append(loss)\n",
    " \n",
    "    # backward pass\n",
    "    net.backward()\n",
    "    \n",
    "    # gradient step\n",
    "    net.step(learning_rate)\n",
    "\n",
    "    # draw the current decision boundary every 250 examples seen\n",
    "    if it % 250 == 0 : \n",
    "        fig, ax = plt.subplots(1, 1, facecolor='#4B6EA9')\n",
    "        ax.set_xlim(x_min, x_max)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "        plot_decision_boundary(ax, x_min, x_max, y_min, y_max, X, Y, net)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, facecolor='#4B6EA9')\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "plot_decision_boundary(ax, x_min, x_max, y_min, y_max, X, Y, net)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title('Evolution of the loss during learning')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('BCE')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
